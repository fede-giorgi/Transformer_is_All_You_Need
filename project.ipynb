{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWY-7_iWEzwl"
      },
      "source": [
        "# Assignment 3: Transformer is All You Need\n",
        "\n",
        "Federico Giorgi (fg2617)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrVJ4E7Ezwn"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DckZ6MKIEzwo"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries\n",
        "import os, math, torch, urllib.request\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Data Preparation"
      ],
      "metadata": {
        "id": "Wvn5fnwPL9ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Load the Tiny Shakespeare text"
      ],
      "metadata": {
        "id": "VMDIvziNMA00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define local path for dataset\n",
        "TINY_PATH = \"tiny_shakespeare.txt\"\n",
        "\n",
        "# Download the Tiny Shakespeare corpus from Karpathyâ€™s repo\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "    TINY_PATH\n",
        ")\n",
        "\n",
        "# Read the corpus into memory as a single string\n",
        "with open(TINY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Print the first 100 characters\n",
        "print(corpus_text[:100])"
      ],
      "metadata": {
        "id": "NOFCpTZyi2-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973bb468-71f9-48d6-95d1-614cbd77dfac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Tokenization"
      ],
      "metadata": {
        "id": "8Gd-yoPcMFxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special tokens for padding, unknown words, and sequence boundaries\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "\n",
        "# Initialize a Byte Pair Encoding (BPE) tokenizer with an unknown token\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Split text initially on whitespace before BPE merges are applied\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the BPE tokenizer on the corpus, limiting vocabulary size to 500 tokens\n",
        "trainer = BpeTrainer(vocab_size=500, min_frequency=2, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator([corpus_text], trainer=trainer)\n",
        "\n",
        "# Retrieve token IDs for the special tokens and vocabulary size\n",
        "pad_id  = tokenizer.token_to_id(\"[PAD]\")\n",
        "bos_id  = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id  = tokenizer.token_to_id(\"[EOS]\")\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# Encode entire corpus to integer IDs\n",
        "ids = tokenizer.encode(corpus_text).ids"
      ],
      "metadata": {
        "id": "DUVdr42OL6tl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Sequence formatting"
      ],
      "metadata": {
        "id": "qPNi6UI_OIGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For next-token prediction: Input = first N tokens, Target = same sequence shifted by 1\n",
        "\n",
        "def make_windows(token_ids, seq_len):\n",
        "    # Total number of tokens in the corpus\n",
        "    L = len(token_ids)\n",
        "\n",
        "    # We need i + seq_len + 1 <= L so that both input and target fit\n",
        "    limit = L - (seq_len + 1)\n",
        "\n",
        "    # Lists to collect all input and target sequences\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Slide a window of size (seq_len + 1) across the corpus with stride 1\n",
        "    for i in range(0, max(0, limit + 1)):\n",
        "        # Take a chunk of length seq_len + 1\n",
        "        seq = token_ids[i : i + seq_len + 1]\n",
        "\n",
        "        # Input: all tokens except the last one\n",
        "        inp = seq[:-1]\n",
        "\n",
        "        # Target: all tokens except the first one (shifted by one position)\n",
        "        tgt = seq[1:]\n",
        "\n",
        "        # Append the pair to the dataset lists\n",
        "        inputs.append(inp)\n",
        "        targets.append(tgt)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "# Generate all (input, target) pairs from the encoded corpus\n",
        "SEQ_LEN = 50\n",
        "inputs, targets = make_windows(ids, SEQ_LEN)"
      ],
      "metadata": {
        "id": "xFfNL--HPOPf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check for the windowed dataset\n",
        "print(f\"Total number of input-target pairs: {len(inputs)}\")\n",
        "\n",
        "# Print the first pair to verify correct shifting\n",
        "print(\"\\nExample pair:\")\n",
        "print(\"Input IDs :\", inputs[0])\n",
        "print(\"Target IDs:\", targets[0])\n",
        "\n",
        "# Decode the first example back to text (optional, for interpretability)\n",
        "print(\"\\nDecoded example:\")\n",
        "print(\"Input text :\", tokenizer.decode(inputs[0]))\n",
        "print(\"Target text:\", tokenizer.decode(targets[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esNl5BCAQ9-6",
        "outputId": "100993c8-b982-4235-9301-ff0e0fa7f12c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of input-target pairs: 448079\n",
            "\n",
            "Example pair:\n",
            "Input IDs : [407, 17, 93, 49, 66, 75, 12, 396, 283, 116, 300, 101, 110, 424, 46, 138, 131, 8, 395, 81, 365, 10, 15, 80, 12, 33, 154, 252, 8, 365, 10, 407, 17, 93, 49, 66, 75, 12, 318, 158, 135, 58, 86, 316, 92, 44, 145, 131, 82, 277]\n",
            "Target IDs: [17, 93, 49, 66, 75, 12, 396, 283, 116, 300, 101, 110, 424, 46, 138, 131, 8, 395, 81, 365, 10, 15, 80, 12, 33, 154, 252, 8, 365, 10, 407, 17, 93, 49, 66, 75, 12, 318, 158, 135, 58, 86, 316, 92, 44, 145, 131, 82, 277, 45]\n",
            "\n",
            "Decoded example:\n",
            "Input text : First C it i z en : Be fore we pro ce ed any f ur ther , hear me speak . A ll : S pe ak , speak . First C it i z en : You are all r es ol ve d ra ther to di\n",
            "Target text: C it i z en : Be fore we pro ce ed any f ur ther , hear me speak . A ll : S pe ak , speak . First C it i z en : You are all r es ol ve d ra ther to di e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 4 Data split"
      ],
      "metadata": {
        "id": "u4uqaaiYPQLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of (input, target) pairs produced by windowing\n",
        "dataset_size = len(inputs)\n",
        "\n",
        "# 80/20 split\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size   = dataset_size - train_size"
      ],
      "metadata": {
        "id": "odrB0gquRx98"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NextTokenDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Simple Dataset wrapper for next-token prediction.\n",
        "    Stores precomputed windows as LongTensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        # Cast each window to torch.LongTensor (needed by nn.Embedding and CE loss)\n",
        "        self.X = [torch.tensor(x, dtype=torch.long) for x in X]\n",
        "        self.Y = [torch.tensor(y, dtype=torch.long) for y in Y]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of (input, target) pairs\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Return one pair shaped (T,), (T,) where T=SEQ_LEN\n",
        "        return self.X[i], self.Y[i]\n",
        "\n",
        "# Full dataset (deterministic split via fixed seed)\n",
        "full_ds = NextTokenDataset(inputs, targets)\n",
        "train_ds, val_ds = random_split(\n",
        "    full_ds,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DataLoader.\n",
        "    Sequences are already fixed-length (SEQ_LEN), so we can stack them directly.\n",
        "\n",
        "    Input:\n",
        "        batch: list of N items, each is (x, y) with shape (T,), (T,)\n",
        "    Output:\n",
        "        X: (B, T), Y: (B, T)\n",
        "    \"\"\"\n",
        "    # b[0] is input tensor, b[1] is target tensor\n",
        "    X = torch.stack([b[0] for b in batch], dim=0)  # (B, T)\n",
        "    Y = torch.stack([b[1] for b in batch], dim=0)  # (B, T)\n",
        "    return X, Y\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# DataLoaders for training and validation.\n",
        "# Note: drop_last=True on train for consistent batch size (helps with certain training loops).\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "# For validation, keep all examples (no shuffling, no drop_last)\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "ngXfQm_NPYYR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity checks\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(f\"Train batches: {len(train_loader)}  |  Val batches: {len(val_loader)}\")\n",
        "print(f\"Batch shapes  -> X: {xb.shape}, Y: {yb.shape}  (expect: ({BATCH_SIZE}, {SEQ_LEN}))\")\n",
        "assert xb.shape == yb.shape == (BATCH_SIZE, SEQ_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_rxsSAbSJSl",
        "outputId": "129154b2-4d1f-4ef5-9572-0430d675fb79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 2800  |  Val batches: 701\n",
            "Batch shapes  -> X: torch.Size([128, 50]), Y: torch.Size([128, 50])  (expect: (128, 50))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Token embedding"
      ],
      "metadata": {
        "id": "WvMtw2tGPUPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 1.5 Token embedding (sinusoidal positions only)\n",
        "\n",
        "# We map token IDs -> dense vectors with nn.Embedding,\n",
        "# then add fixed (non-trainable) sinusoidal positional encodings Ã  la \"Attention Is All You Need\".\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token embedding table: converts token IDs to d_model-dimensional vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T) of dtype long\n",
        "        return self.embed(x)  # (B, T, d_model)\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Fixed sinusoidal positional encodings (no learned parameters).\n",
        "    Follows Vaswani et al. (2017). Register as a buffer so it moves with .to(device)\n",
        "    but is excluded from optimizer updates.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 10000):\n",
        "        super().__init__()\n",
        "        # Create table of shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)        # (max_len, 1)\n",
        "        # Frequencies for even/odd dimensions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "\n",
        "        # Register as buffer (not a Parameter): no gradients, moves with the module across devices\n",
        "        self.register_buffer(\"pe\", pe)  # shape: (max_len, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add positional encodings for the first T positions.\n",
        "        x is expected to be (B, T, d_model).\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        # Slice the first T rows, then broadcast over batch dimension\n",
        "        return x + self.pe[:T].unsqueeze(0)  # (1, T, D) + (B, T, D) -> (B, T, D)\n",
        "\n",
        "\n",
        "# ----- build embeddings and run a quick test on one batch -----\n",
        "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "d_model  = 256\n",
        "max_len  = max(SEQ_LEN, 512)  # keep some headroom if you later increase context length\n",
        "\n",
        "tok_emb  = TokenEmbedding(vocab_size, d_model).to(device)\n",
        "pos_enc  = SinusoidalPositionalEncoding(d_model, max_len=max_len).to(device)\n",
        "\n",
        "# Fetch one batch of IDs (B, T)\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb = xb.to(device)  # (B, T)\n",
        "\n",
        "# Token embeddings: (B, T, d_model)\n",
        "h_tok = tok_emb(xb)\n",
        "\n",
        "# Add sinusoidal positions: (B, T, d_model)\n",
        "h = pos_enc(h_tok)"
      ],
      "metadata": {
        "id": "3Fz4gMLAhjxh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sanity checks for embeddings + sinusoidal positions:\")\n",
        "print(\" - xb dtype / shape:\", xb.dtype, xb.shape)\n",
        "print(\" - token only     :\", h_tok.shape)\n",
        "print(\" - token+position :\", h.shape)\n",
        "\n",
        "# 1) Shapes must match\n",
        "assert h_tok.shape == (xb.size(0), xb.size(1), d_model)\n",
        "assert h.shape     == (xb.size(0), xb.size(1), d_model)\n",
        "\n",
        "# 2) Positional buffer should not require gradients\n",
        "assert pos_enc.pe.requires_grad is False\n",
        "\n",
        "# 3) Same position across different sequences has identical positional vector\n",
        "#    (i.e., h[b, t, :] - h_tok[b, t, :] equals the same vector for all b, fixed t)\n",
        "with torch.no_grad():\n",
        "    t = 0  # check the first position; you can try other t as well\n",
        "    pos_vecs = h[:, t, :] - h_tok[:, t, :]              # (B, D)\n",
        "    diff = (pos_vecs - pos_vecs[0]).abs().max().item()  # max deviation from the first sample\n",
        "    print(f\" - max deviation of pos[t={t}] across batch: {diff:.3e}\")\n",
        "    assert diff < 1e-6\n",
        "\n",
        "# 4) Different positions within the same sequence must have different positional vectors\n",
        "with torch.no_grad():\n",
        "    if xb.size(1) >= 2:\n",
        "        # Compare pos enc at t=0 vs t=1 for the first sample in batch\n",
        "        v0 = h[0, 0, :] - h_tok[0, 0, :]\n",
        "        v1 = h[0, 1, :] - h_tok[0, 1, :]\n",
        "        cosine = torch.nn.functional.cosine_similarity(v0, v1, dim=0).item()\n",
        "        print(f\" - cosine similarity between pos[0] and pos[1]: {cosine:.4f} (should be < 1.0)\")"
      ],
      "metadata": {
        "id": "TYicIHuucuSb",
        "outputId": "298da490-98e8-4035-8ce8-fda27f9d597c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks for embeddings + sinusoidal positions:\n",
            " - xb dtype / shape: torch.int64 torch.Size([128, 50])\n",
            " - token only     : torch.Size([128, 50, 256])\n",
            " - token+position : torch.Size([128, 50, 256])\n",
            " - max deviation of pos[t=0] across batch: 2.384e-07\n",
            " - cosine similarity between pos[0] and pos[1]: 0.9721 (should be < 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Tiny Transformer Implementation"
      ],
      "metadata": {
        "id": "-msA__kNPerv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention with causal masking\"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        scores = Q @ K.transpose(-2, -1) / (d_model ** 0.5)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
        "\n",
        "        output = attn_weights @ V\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Two-layer MLP\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"One transformer block: Attention + FFN + Residuals\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.attention = SelfAttention(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, attn_weights = self.attention(x)\n",
        "        x = x + attn_out  # Residual\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = x + ffn_out  # Residual\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"Complete transformer model\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_layers, d_ff, max_seq_len=128):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, max_seq_len, d_model) * 0.02)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, d_ff) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        x = self.token_embedding(x) + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        all_attn = []\n",
        "        for block in self.blocks:\n",
        "            x, attn = block(x)\n",
        "            all_attn.append(attn)\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "        return logits, all_attn"
      ],
      "metadata": {
        "id": "SaL12KfziXcJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Visualization & Interpretation"
      ],
      "metadata": {
        "id": "CTtm-_smaueJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print(f\"âœ“ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš  No GPU found, using CPU\")\n",
        "\n",
        "d_model = 128\n",
        "n_layers = 2\n",
        "d_ff = 512\n",
        "learning_rate = 0.001\n",
        "n_epochs = 1000\n",
        "batch_size = 4096\n",
        "\n",
        "model = TinyTransformer(vocab_size, d_model, n_layers, d_ff, max_len).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)\n",
        "\n",
        "X_padded = X_padded.to(device)\n",
        "Y_tensor = Y_tensor.to(device)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Embedding dim: {d_model}\")\n",
        "print(f\"Layers: {n_layers}\")\n",
        "print(f\"FFN hidden: {d_ff}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Epochs: {n_epochs}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "best_loss = float('inf')\n",
        "n_samples = len(X_padded)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    indices = torch.randperm(n_samples, device=device)\n",
        "\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        batch_idx = indices[i:min(i+batch_size, n_samples)]\n",
        "        batch_X = X_padded[batch_idx]\n",
        "        batch_Y = Y_tensor[batch_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            logits, _ = model(batch_X)\n",
        "            predictions = logits[:, -1, :]\n",
        "            loss = criterion(predictions, batch_Y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        pred_tokens = torch.argmax(predictions, dim=1)\n",
        "        acc = (pred_tokens == batch_Y).float().mean().item()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "        n_batches += 1\n",
        "\n",
        "    avg_loss = epoch_loss / n_batches\n",
        "    avg_acc = epoch_acc / n_batches\n",
        "\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accs.append(avg_acc)\n",
        "\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(avg_loss)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    if new_lr != old_lr:\n",
        "        print(f\"    â†’ LR reduced: {old_lr:.6f} â†’ {new_lr:.6f}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        best_epoch = epoch + 1\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1:4d}/{n_epochs} | Loss: {avg_loss:.4f} | Acc: {avg_acc:.2%}\")\n",
        "        if device.type == 'cuda':\n",
        "            print(f\"    GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
        "\n",
        "    if avg_loss < 0.01:\n",
        "        print(f\"\\nâœ“ Excellent! Stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nâœ“ Training complete!\")\n",
        "print(f\"Best: Loss={best_loss:.4f} at epoch {best_epoch}\")\n",
        "print(f\"Final: Loss={train_losses[-1]:.4f}, Acc={train_accs[-1]:.2%}\")\n",
        "\n",
        "# æ¸…ç†GPUç¼“å­˜\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Visualization: Training Progress\n",
        "# ============================================================\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(train_losses, linewidth=2, color='steelblue')\n",
        "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2.plot(train_accs, linewidth=2, color='seagreen')\n",
        "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylim([0, 1])\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 6. Inference\n",
        "# ============================================================\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_next(prompt, top_k=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = [word_to_id[w] for w in prompt.lower().split() if w in word_to_id]\n",
        "        if not tokens:\n",
        "            return []\n",
        "\n",
        "        input_tensor = torch.tensor([tokens])\n",
        "        logits, _ = model(input_tensor)\n",
        "        probs = F.softmax(logits[0, -1, :], dim=0)\n",
        "\n",
        "        top_probs, top_idx = torch.topk(probs, min(top_k, vocab_size))\n",
        "        return [(id_to_word[i.item()], p.item()) for i, p in zip(top_idx, top_probs)]\n",
        "\n",
        "# Test with predefined prompts\n",
        "test_prompts = [\n",
        "    \"the sun rises\",\n",
        "    \"birds fly south\",\n",
        "    \"water boils at\",\n",
        "    \"plants need sunlight\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INFERENCE: NEXT TOKEN PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    preds = predict_next(prompt)\n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "    for i, (word, prob) in enumerate(preds, 1):\n",
        "        bar = 'â–ˆ' * int(prob * 40)\n",
        "        print(f\"  {i}. {word:15s} {prob:.3f} {bar}\")\n",
        "\n",
        "# Create interactive widget\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTERACTIVE NEXT TOKEN PREDICTOR\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create input text box\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter text here...',\n",
        "    description='Input:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "# Create slider for top-k\n",
        "topk_slider = widgets.IntSlider(\n",
        "    value=5,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    step=1,\n",
        "    description='Top-K:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Create output area\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Create button for prediction\n",
        "predict_button = widgets.Button(\n",
        "    description='Predict Next Token',\n",
        "    button_style='primary',\n",
        "    tooltip='Click to predict next token'\n",
        ")\n",
        "\n",
        "def on_predict(b=None):\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        prompt = text_input.value.strip()\n",
        "\n",
        "        if not prompt:\n",
        "            print(\"âš ï¸ Please enter some text!\")\n",
        "            return\n",
        "\n",
        "        top_k = topk_slider.value\n",
        "        preds = predict_next(prompt, top_k)\n",
        "\n",
        "        if not preds:\n",
        "            print(\"âŒ No valid words found in vocabulary!\")\n",
        "            return\n",
        "\n",
        "        # Display results with nice formatting\n",
        "        print(f\"ðŸ“ Input: '{prompt}'\\n\")\n",
        "        print(\"ðŸŽ¯ Next Token Predictions:\\n\")\n",
        "\n",
        "        # Find max probability for scaling bars\n",
        "        max_prob = preds[0][1] if preds else 1\n",
        "\n",
        "        for i, (word, prob) in enumerate(preds, 1):\n",
        "            # Create probability bar\n",
        "            bar_length = int(prob / max_prob * 30)\n",
        "            bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)\n",
        "\n",
        "            # Color coding based on probability\n",
        "            if prob > 0.5:\n",
        "                emoji = \"ðŸŸ¢\"\n",
        "            elif prob > 0.2:\n",
        "                emoji = \"ðŸŸ¡\"\n",
        "            else:\n",
        "                emoji = \"ðŸ”´\"\n",
        "\n",
        "            print(f\"  {emoji} {i:2d}. {word:15s} [{bar}] {prob:.3%}\")\n",
        "\n",
        "        print(f\"\\nðŸ’¡ Tip: The model predicts '{preds[0][0]}' as the most likely next token\")\n",
        "\n",
        "# Bind button click event\n",
        "predict_button.on_click(on_predict)\n",
        "\n",
        "# Also trigger prediction when pressing Enter in text box\n",
        "def on_text_change(change):\n",
        "    if change['name'] == 'value' and change['new'] and change['new'][-1:] == '\\n':\n",
        "        text_input.value = text_input.value.strip()\n",
        "        on_predict()\n",
        "\n",
        "text_input.observe(on_text_change)\n",
        "\n",
        "# Create layout\n",
        "title = widgets.HTML(value=\"<h3>ðŸ¤– Interactive Next Token Predictor</h3>\")\n",
        "instructions = widgets.HTML(value=\"<p><i>Type a sentence fragment and see what the model predicts as the next token!</i></p>\")\n",
        "\n",
        "# Arrange widgets\n",
        "input_box = widgets.HBox([text_input, predict_button])\n",
        "controls = widgets.VBox([topk_slider])\n",
        "\n",
        "# Display everything\n",
        "display(title)\n",
        "display(instructions)\n",
        "display(input_box)\n",
        "display(controls)\n",
        "display(output_area)\n",
        "\n",
        "# Show initial example\n",
        "text_input.value = \"the sun\"\n",
        "on_predict()"
      ],
      "metadata": {
        "id": "NEXfmXg0avlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}