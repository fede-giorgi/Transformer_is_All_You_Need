{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWY-7_iWEzwl"
      },
      "source": [
        "# Assignment 3: Transformer is All You Need\n",
        "\n",
        "Federico Giorgi (fg2617)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrVJ4E7Ezwn"
      },
      "source": [
        "## 1 Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DckZ6MKIEzwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6021c96c-1502-4b0f-9f40-64967a38cd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import all the libraries\n",
        "import os, math, torch, urllib.request\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel, Whitespace\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Data Preparation"
      ],
      "metadata": {
        "id": "Wvn5fnwPL9ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Corpus loading\n",
        "\n",
        "The Tiny Shakespeare corpus is downloaded directly from Karpathy’s repository and loaded into memory as a single UTF-8 string. Printing the initial characters serves as a basic integrity check to ensure the file was retrieved correctly."
      ],
      "metadata": {
        "id": "VMDIvziNMA00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define local path for dataset\n",
        "TINY_PATH = \"tiny_shakespeare.txt\"\n",
        "\n",
        "# Download the Tiny Shakespeare corpus from Karpathy’s repo\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "    TINY_PATH\n",
        ")\n",
        "\n",
        "# Read the corpus into memory as a single string\n",
        "with open(TINY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Print the first 100 characters\n",
        "print(corpus_text[:100])"
      ],
      "metadata": {
        "id": "NOFCpTZyi2-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6442064b-43ae-49d4-ca44-672da61975ec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Subword tokenization\n",
        "\n",
        "A Byte Pair Encoding (BPE) tokenizer with a vocabulary capped at 500 tokens is trained on the corpus. Special tokens for padding, unknown symbols, and sequence boundaries are included. After training, the entire corpus is converted into integer token ids, forming the numerical representation used by the model."
      ],
      "metadata": {
        "id": "8Gd-yoPcMFxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special tokens for padding, unknown words, and sequence boundaries\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "\n",
        "# Initialize a Byte Pair Encoding (BPE) tokenizer with an unknown token\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Use GPT-style ByteLevel pre-tokenization (byte-wise mapping + space normalization)\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "# Train the BPE tokenizer on the corpus, limiting vocabulary size to 500 tokens\n",
        "trainer = BpeTrainer(vocab_size=500, min_frequency=2, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator([corpus_text], trainer=trainer)\n",
        "\n",
        "# Retrieve token IDs for the special tokens and vocabulary size\n",
        "pad_id  = tokenizer.token_to_id(\"[PAD]\")\n",
        "bos_id  = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id  = tokenizer.token_to_id(\"[EOS]\")\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# Encode entire corpus to integer IDs\n",
        "ids = tokenizer.encode(corpus_text).ids"
      ],
      "metadata": {
        "id": "DUVdr42OL6tl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sanity check 1: encode/decode consistency\")\n",
        "\n",
        "for start in [0, 1000, 5000]:\n",
        "    snippet = corpus_text[start:start+120]\n",
        "    decoded = tokenizer.decode(tokenizer.encode(snippet).ids)\n",
        "\n",
        "    print(\"\\n--- Original ---\")\n",
        "    print(snippet)\n",
        "\n",
        "    print(\"\\n--- Decoded ---\")\n",
        "    print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6oAfJAEndDm",
        "outputId": "cf5f772a-140e-4c22-e28f-699eb5b82cba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check 1: encode/decode consistency\n",
            "\n",
            "--- Original ---\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved ra\n",
            "\n",
            "--- Decoded ---\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved ra\n",
            "\n",
            "--- Original ---\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the comm\n",
            "\n",
            "--- Decoded ---\n",
            " Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the comm\n",
            "\n",
            "--- Original ---\n",
            "or heart, the arm our soldier,\n",
            "Our steed the leg, the tongue our trumpeter.\n",
            "With other muniments and petty helps\n",
            "In this\n",
            "\n",
            "--- Decoded ---\n",
            " or heart, the arm our soldier,\n",
            "Our steed the leg, the tongue our trumpeter.\n",
            "With other muniments and petty helps\n",
            "In this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSanity check 2: ID preservation check\")\n",
        "\n",
        "snippet = corpus_text[2000:2200]\n",
        "\n",
        "ids_1   = tokenizer.encode(snippet).ids\n",
        "decoded = tokenizer.decode(ids_1)\n",
        "ids_2   = tokenizer.encode(decoded).ids\n",
        "\n",
        "print(\"IDs identical? \", ids_1 == ids_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GYTzeulpfXA",
        "outputId": "fecd44b1-3455-4195-af02-ae28de56b681"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sanity check 2: ID preservation check\n",
            "IDs identical?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Sequence formatting\n",
        "\n",
        "For next-token prediction, the tokenized text is split into overlapping fixed-length windows.\n",
        "Each window provides an input sequence of length T and a corresponding target sequence shifted by one position. This produces many short supervised examples capturing local context within the corpus."
      ],
      "metadata": {
        "id": "qPNi6UI_OIGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For next-token prediction: Input = first N tokens, Target = same sequence shifted by 1\n",
        "\n",
        "def make_windows(token_ids, seq_len):\n",
        "    # Total number of tokens in the corpus\n",
        "    L = len(token_ids)\n",
        "\n",
        "    # We need i + seq_len + 1 <= L so that both input and target fit\n",
        "    limit = L - (seq_len + 1)\n",
        "\n",
        "    # Lists to collect all input and target sequences\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Slide a window of size (seq_len + 1) across the corpus with stride 1\n",
        "    for i in range(0, max(0, limit + 1)):\n",
        "        # Take a chunk of length seq_len + 1\n",
        "        seq = token_ids[i : i + seq_len + 1]\n",
        "\n",
        "        # Input: all tokens except the last one\n",
        "        inp = seq[:-1]\n",
        "\n",
        "        # Target: all tokens except the first one (shifted by one position)\n",
        "        tgt = seq[1:]\n",
        "\n",
        "        # Append the pair to the dataset lists\n",
        "        inputs.append(inp)\n",
        "        targets.append(tgt)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "# Generate all (input, target) pairs from the encoded corpus\n",
        "SEQ_LEN = 50\n",
        "inputs, targets = make_windows(ids, SEQ_LEN)"
      ],
      "metadata": {
        "id": "xFfNL--HPOPf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check for the windowed dataset\n",
        "print(f\"Total number of input-target pairs: {len(inputs)}\")\n",
        "\n",
        "# Print the first pair to verify correct shifting\n",
        "print(\"\\nExample pair:\")\n",
        "print(\"Input IDs :\", inputs[0])\n",
        "print(\"Target IDs:\", targets[0])\n",
        "\n",
        "# Decode the first example back to text (optional, for interpretability)\n",
        "print(\"\\nDecoded example:\")\n",
        "print(\"Input text :\", tokenizer.decode(inputs[0]))\n",
        "print(\"Target text:\", tokenizer.decode(targets[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esNl5BCAQ9-6",
        "outputId": "1d94c3b9-5872-4626-fbcb-204879a18fb4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of input-target pairs: 517133\n",
            "\n",
            "Example pair:\n",
            "Input IDs : [68, 484, 233, 87, 49, 66, 92, 12, 67, 16, 45, 361, 144, 397, 121, 128, 216, 65, 84, 174, 60, 148, 8, 487, 130, 429, 10, 67, 67, 15, 86, 12, 67, 33, 56, 393, 8, 429, 10, 67, 67, 484, 233, 87, 49, 66, 92, 12, 67, 378]\n",
            "Target IDs: [484, 233, 87, 49, 66, 92, 12, 67, 16, 45, 361, 144, 397, 121, 128, 216, 65, 84, 174, 60, 148, 8, 487, 130, 429, 10, 67, 67, 15, 86, 12, 67, 33, 56, 393, 8, 429, 10, 67, 67, 484, 233, 87, 49, 66, 92, 12, 67, 378, 231]\n",
            "\n",
            "Decoded example:\n",
            "Input text :  First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "Target text: First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Train-validation split\n",
        "\n",
        "The full set of (input, target) pairs is wrapped in a PyTorch Dataset and deterministically divided into an 80/20 train–validation split. Because all sequences have fixed length, batches are created by stacking tensors directly without padding."
      ],
      "metadata": {
        "id": "u4uqaaiYPQLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of (input, target) pairs produced by windowing\n",
        "dataset_size = len(inputs)\n",
        "\n",
        "# 80/20 split\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size   = dataset_size - train_size\n",
        "print(f\"Train size: {train_size} | Val size: {val_size}\")"
      ],
      "metadata": {
        "id": "odrB0gquRx98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a6d74c-1d16-42b5-d472-e86ff4505fd3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 413706 | Val size: 103427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NextTokenDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Simple Dataset wrapper for next-token prediction.\n",
        "    Stores precomputed windows as LongTensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        # Cast each window to torch.LongTensor (needed by nn.Embedding and CE loss)\n",
        "        self.X = [torch.tensor(x, dtype=torch.long) for x in X]\n",
        "        self.Y = [torch.tensor(y, dtype=torch.long) for y in Y]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of (input, target) pairs\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Return one pair shaped (T,), (T,) where T=SEQ_LEN\n",
        "        return self.X[i], self.Y[i]\n",
        "\n",
        "# Full dataset (deterministic split via fixed seed)\n",
        "full_ds = NextTokenDataset(inputs, targets)\n",
        "train_ds, val_ds = random_split(\n",
        "    full_ds,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DataLoader.\n",
        "    Sequences are already fixed-length (SEQ_LEN), so we can stack them directly.\n",
        "\n",
        "    Input:\n",
        "        batch: list of N items, each is (x, y) with shape (T,), (T,)\n",
        "    Output:\n",
        "        X: (B, T), Y: (B, T)\n",
        "    \"\"\"\n",
        "    # b[0] is input tensor, b[1] is target tensor\n",
        "    X = torch.stack([b[0] for b in batch], dim=0)  # (B, T)\n",
        "    Y = torch.stack([b[1] for b in batch], dim=0)  # (B, T)\n",
        "    return X, Y\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# DataLoaders for training and validation.\n",
        "# Note: drop_last=True on train for consistent batch size (helps with certain training loops).\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "# For validation, keep all examples (no shuffling, no drop_last)\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "ngXfQm_NPYYR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity checks\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(f\"Train batches: {len(train_loader)}  |  Val batches: {len(val_loader)}\")\n",
        "print(f\"Batch shapes  -> X: {xb.shape}, Y: {yb.shape}  (expect: ({BATCH_SIZE}, {SEQ_LEN}))\")\n",
        "assert xb.shape == yb.shape == (BATCH_SIZE, SEQ_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_rxsSAbSJSl",
        "outputId": "27a5c1ab-9ad0-427a-ba3c-136cabae08b6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 3232  |  Val batches: 809\n",
            "Batch shapes  -> X: torch.Size([128, 50]), Y: torch.Size([128, 50])  (expect: (128, 50))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Token embedding\n",
        "\n",
        "Token ids are mapped to continuous vectors through an nn.Embedding layer, producing representations of shape (B,T,d_model). These embeddings form the model’s initial hidden states."
      ],
      "metadata": {
        "id": "WvMtw2tGPUPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 1.5 Token embedding (sinusoidal positions only)\n",
        "\n",
        "# We map token IDs -> dense vectors with nn.Embedding,\n",
        "# then add fixed (non-trainable) sinusoidal positional encodings à la \"Attention Is All You Need\".\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token embedding table: converts token IDs to d_model-dimensional vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T) of dtype long\n",
        "        return self.embed(x)  # (B, T, d_model)\n",
        "\n",
        "\n",
        "class SinusoidalEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Fixed sinusoidal positional encodings (no learned parameters).\n",
        "    Follows Vaswani et al. (2017). Register as a buffer so it moves with .to(device)\n",
        "    but is excluded from optimizer updates.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 10000):\n",
        "        super().__init__()\n",
        "        # Create table of shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)        # (max_len, 1)\n",
        "        # Frequencies for even/odd dimensions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "\n",
        "        # Register as buffer (not a Parameter): no gradients, moves with the module across devices\n",
        "        self.register_buffer(\"pe\", pe)  # shape: (max_len, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add positional encodings for the first T positions.\n",
        "        x is expected to be (B, T, d_model).\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        # Slice the first T rows, then broadcast over batch dimension\n",
        "        return x + self.pe[:T].unsqueeze(0)  # (1, T, D) + (B, T, D) -> (B, T, D)\n",
        "\n",
        "\n",
        "# ----- build embeddings and run a quick test on one batch -----\n",
        "d_model  = 256\n",
        "max_len  = max(SEQ_LEN, 512)  # keep some headroom if you later increase context length\n",
        "\n",
        "tok_emb  = TokenEmbedding(vocab_size, d_model).to(device)\n",
        "pos_enc  = SinusoidalEmbeddings(d_model, max_len=max_len).to(device)\n",
        "\n",
        "# Fetch one batch of IDs (B, T)\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb = xb.to(device)  # (B, T)\n",
        "\n",
        "# Token embeddings: (B, T, d_model)\n",
        "h_tok = tok_emb(xb)\n",
        "\n",
        "# Add sinusoidal positions: (B, T, d_model)\n",
        "h = pos_enc(h_tok)"
      ],
      "metadata": {
        "id": "3Fz4gMLAhjxh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sanity checks for embeddings + sinusoidal positions:\")\n",
        "print(\" - xb dtype / shape:\", xb.dtype, xb.shape)\n",
        "print(\" - token only     :\", h_tok.shape)\n",
        "print(\" - token+position :\", h.shape)\n",
        "\n",
        "# 1) Shapes must match\n",
        "assert h_tok.shape == (xb.size(0), xb.size(1), d_model)\n",
        "assert h.shape     == (xb.size(0), xb.size(1), d_model)\n",
        "\n",
        "# 2) Positional buffer should not require gradients\n",
        "assert pos_enc.pe.requires_grad is False\n",
        "\n",
        "# 3) Same position across different sequences has identical positional vector\n",
        "#    (i.e., h[b, t, :] - h_tok[b, t, :] equals the same vector for all b, fixed t)\n",
        "with torch.no_grad():\n",
        "    t = 0  # check the first position; you can try other t as well\n",
        "    pos_vecs = h[:, t, :] - h_tok[:, t, :]              # (B, D)\n",
        "    diff = (pos_vecs - pos_vecs[0]).abs().max().item()  # max deviation from the first sample\n",
        "    print(f\" - max deviation of pos[t={t}] across batch: {diff:.3e}\")\n",
        "    assert diff < 1e-6\n",
        "\n",
        "# 4) Different positions within the same sequence must have different positional vectors\n",
        "with torch.no_grad():\n",
        "    if xb.size(1) >= 2:\n",
        "        # Compare pos enc at t=0 vs t=1 for the first sample in batch\n",
        "        v0 = h[0, 0, :] - h_tok[0, 0, :]\n",
        "        v1 = h[0, 1, :] - h_tok[0, 1, :]\n",
        "        cosine = torch.nn.functional.cosine_similarity(v0, v1, dim=0).item()\n",
        "        print(f\" - cosine similarity between pos[0] and pos[1]: {cosine:.4f} (should be < 1.0)\")"
      ],
      "metadata": {
        "id": "TYicIHuucuSb",
        "outputId": "157b9418-3191-4c11-840c-ba0bf3f3f0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks for embeddings + sinusoidal positions:\n",
            " - xb dtype / shape: torch.int64 torch.Size([128, 50])\n",
            " - token only     : torch.Size([128, 50, 256])\n",
            " - token+position : torch.Size([128, 50, 256])\n",
            " - max deviation of pos[t=0] across batch: 2.980e-07\n",
            " - cosine similarity between pos[0] and pos[1]: 0.9721 (should be < 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Tiny Transformer Implementation"
      ],
      "metadata": {
        "id": "-msA__kNPerv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Transformer Structure"
      ],
      "metadata": {
        "id": "YvEVjqIfgWLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention with causal masking\"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        scores = Q @ K.transpose(-2, -1) / (d_model ** 0.5)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
        "\n",
        "        output = attn_weights @ V\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Two-layer MLP\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"One transformer block: Attention + FFN + Residuals\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.attention = SelfAttention(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # RMSNorm\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm attention\n",
        "        h = self.norm1(x)\n",
        "        attn_out, attn_weights = self.attention(h)\n",
        "        x = x + attn_out   # residual connection\n",
        "\n",
        "        # Pre-norm feed-forward\n",
        "        h2 = self.norm2(x)\n",
        "        ffn_out = self.ffn(h2)\n",
        "        x = x + ffn_out    # residual connection\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"Complete transformer model\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_layers, d_ff, max_seq_len=128):\n",
        "        super().__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_encoding = SinusoidalEmbeddings(\n",
        "            d_model=d_model,\n",
        "            max_len=max_seq_len\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, d_ff) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Token embeddings: (B, T, d_model)\n",
        "        x = self.token_embedding(x)\n",
        "\n",
        "        # Add fixed sinusoidal positional encodings: still (B, T, d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        all_attn = []\n",
        "        for block in self.blocks:\n",
        "            x, attn = block(x)\n",
        "            all_attn.append(attn)\n",
        "\n",
        "        logits = self.output_layer(x)  # (B, T, vocab_size)\n",
        "        return logits, all_attn"
      ],
      "metadata": {
        "id": "SaL12KfziXcJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Training procedure"
      ],
      "metadata": {
        "id": "MWu-27bZgaYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 128\n",
        "n_layers = 2\n",
        "d_ff = 512\n",
        "learning_rate = 1e-3\n",
        "n_epochs = 50\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "model = TinyTransformer(vocab_size, d_model, n_layers, d_ff, max_seq_len=SEQ_LEN).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "use_amp = device.type == \"cuda\"\n",
        "scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Embedding dim: {d_model}\")\n",
        "print(f\"Layers: {n_layers}\")\n",
        "print(f\"FFN hidden: {d_ff}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Epochs: {n_epochs}\")\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_ppl_list = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)            # (B, T)\n",
        "        yb = yb.to(device)            # (B, T)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(enabled=use_amp):\n",
        "            logits, _ = model(xb)     # (B, T, vocab_size)\n",
        "            loss = criterion(\n",
        "                logits.view(-1, vocab_size),\n",
        "                yb.view(-1)\n",
        "            )\n",
        "\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits, _ = model(xb)\n",
        "            loss = criterion(\n",
        "                logits.view(-1, vocab_size),\n",
        "                yb.view(-1)\n",
        "            )\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    val_ppl = math.exp(avg_val_loss)\n",
        "    val_ppl_list.append(val_ppl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:3d}/{n_epochs} | train loss: {avg_train_loss:.4f} | val loss: {avg_val_loss:.4f} | val PPL: {val_ppl:.2f}\")\n",
        "\n",
        "print(\"\\nTraining finished!\")\n",
        "\n",
        "final_val_loss = val_losses[-1]\n",
        "final_val_ppl = math.exp(final_val_loss)\n",
        "print(f\"\\nFinal validation loss: {final_val_loss:.4f}\")\n",
        "print(f\"Final validation perplexity: {final_val_ppl:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEXfmXg0avlX",
        "outputId": "4d394e5f-250f-47a3-9cfb-5e64f2d1f82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "MODEL ARCHITECTURE\n",
            "======================================================================\n",
            "Embedding dim: 128\n",
            "Layers: 2\n",
            "FFN hidden: 512\n",
            "Batch size: 128\n",
            "Parameters: 490,740\n",
            "Epochs: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-763539771.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=use_amp)\n",
            "/tmp/ipython-input-763539771.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/50 | train loss: 2.9285 | val loss: 2.5790 | val PPL: 13.18\n",
            "Epoch   2/50 | train loss: 2.4569 | val loss: 2.3611 | val PPL: 10.60\n",
            "Epoch   3/50 | train loss: 2.2917 | val loss: 2.2459 | val PPL: 9.45\n",
            "Epoch   4/50 | train loss: 2.1946 | val loss: 2.1737 | val PPL: 8.79\n",
            "Epoch   5/50 | train loss: 2.1290 | val loss: 2.1171 | val PPL: 8.31\n",
            "Epoch   6/50 | train loss: 2.0809 | val loss: 2.0796 | val PPL: 8.00\n",
            "Epoch   7/50 | train loss: 2.0433 | val loss: 2.0456 | val PPL: 7.73\n",
            "Epoch   8/50 | train loss: 2.0126 | val loss: 2.0139 | val PPL: 7.49\n",
            "Epoch   9/50 | train loss: 1.9876 | val loss: 1.9910 | val PPL: 7.32\n",
            "Epoch  10/50 | train loss: 1.9669 | val loss: 1.9779 | val PPL: 7.23\n",
            "Epoch  11/50 | train loss: 1.9491 | val loss: 1.9602 | val PPL: 7.10\n",
            "Epoch  12/50 | train loss: 1.9333 | val loss: 1.9447 | val PPL: 6.99\n",
            "Epoch  13/50 | train loss: 1.9200 | val loss: 1.9292 | val PPL: 6.88\n",
            "Epoch  14/50 | train loss: 1.9079 | val loss: 1.9163 | val PPL: 6.80\n",
            "Epoch  15/50 | train loss: 1.8973 | val loss: 1.9103 | val PPL: 6.76\n",
            "Epoch  16/50 | train loss: 1.8879 | val loss: 1.8993 | val PPL: 6.68\n",
            "Epoch  17/50 | train loss: 1.8789 | val loss: 1.8941 | val PPL: 6.65\n",
            "Epoch  18/50 | train loss: 1.8710 | val loss: 1.8859 | val PPL: 6.59\n",
            "Epoch  19/50 | train loss: 1.8635 | val loss: 1.8751 | val PPL: 6.52\n",
            "Epoch  20/50 | train loss: 1.8569 | val loss: 1.8748 | val PPL: 6.52\n",
            "Epoch  21/50 | train loss: 1.8506 | val loss: 1.8704 | val PPL: 6.49\n",
            "Epoch  22/50 | train loss: 1.8440 | val loss: 1.8560 | val PPL: 6.40\n",
            "Epoch  23/50 | train loss: 1.8391 | val loss: 1.8550 | val PPL: 6.39\n",
            "Epoch  24/50 | train loss: 1.8336 | val loss: 1.8503 | val PPL: 6.36\n",
            "Epoch  25/50 | train loss: 1.8294 | val loss: 1.8466 | val PPL: 6.34\n",
            "Epoch  26/50 | train loss: 1.8252 | val loss: 1.8445 | val PPL: 6.32\n",
            "Epoch  27/50 | train loss: 1.8205 | val loss: 1.8371 | val PPL: 6.28\n",
            "Epoch  28/50 | train loss: 1.8168 | val loss: 1.8319 | val PPL: 6.25\n",
            "Epoch  29/50 | train loss: 1.8129 | val loss: 1.8290 | val PPL: 6.23\n",
            "Epoch  30/50 | train loss: 1.8096 | val loss: 1.8287 | val PPL: 6.23\n",
            "Epoch  31/50 | train loss: 1.8063 | val loss: 1.8252 | val PPL: 6.20\n",
            "Epoch  32/50 | train loss: 1.8027 | val loss: 1.8238 | val PPL: 6.20\n",
            "Epoch  33/50 | train loss: 1.8003 | val loss: 1.8156 | val PPL: 6.14\n",
            "Epoch  34/50 | train loss: 1.7972 | val loss: 1.8109 | val PPL: 6.12\n",
            "Epoch  35/50 | train loss: 1.7945 | val loss: 1.8168 | val PPL: 6.15\n",
            "Epoch  36/50 | train loss: 1.7918 | val loss: 1.8095 | val PPL: 6.11\n",
            "Epoch  37/50 | train loss: 1.7896 | val loss: 1.8115 | val PPL: 6.12\n",
            "Epoch  38/50 | train loss: 1.7872 | val loss: 1.8082 | val PPL: 6.10\n",
            "Epoch  39/50 | train loss: 1.7851 | val loss: 1.8064 | val PPL: 6.09\n",
            "Epoch  40/50 | train loss: 1.7827 | val loss: 1.7986 | val PPL: 6.04\n",
            "Epoch  41/50 | train loss: 1.7804 | val loss: 1.7992 | val PPL: 6.04\n",
            "Epoch  42/50 | train loss: 1.7782 | val loss: 1.8029 | val PPL: 6.07\n",
            "Epoch  43/50 | train loss: 1.7766 | val loss: 1.7985 | val PPL: 6.04\n",
            "Epoch  44/50 | train loss: 1.7742 | val loss: 1.7943 | val PPL: 6.02\n",
            "Epoch  45/50 | train loss: 1.7730 | val loss: 1.7942 | val PPL: 6.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Visualization & Interpretation"
      ],
      "metadata": {
        "id": "CTtm-_smaueJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Training and validation loss curves"
      ],
      "metadata": {
        "id": "PgxvBLZpgeX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(train_losses, label=\"Train loss\")\n",
        "plt.plot(val_losses, label=\"Val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w8p0MRFqc8OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Attention Heatmap"
      ],
      "metadata": {
        "id": "q3-smMdO659u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(layer_idx):\n",
        "  model.eval()\n",
        "\n",
        "  # Take one batch from the validation loader\n",
        "  xb, yb = next(iter(val_loader))\n",
        "  xb = xb.to(device)\n",
        "\n",
        "  # Forward pass with attention collection enabled\n",
        "  with torch.no_grad():\n",
        "      logits, all_attn = model(xb)\n",
        "\n",
        "  # Select the first sequence in the batch\n",
        "  seq_tokens = xb[0] # shape: (T,)\n",
        "  seq_tokens_cpu = seq_tokens.cpu().tolist()\n",
        "\n",
        "  # Choose which layer’s attention to visualize (e.g., first layer)\n",
        "  attn = all_attn[layer_idx][0]    # shape: (T, T) for the first example\n",
        "  attn = attn.detach().cpu().numpy()\n",
        "\n",
        "  # Plot the attention matrix\n",
        "  plt.figure(figsize=(7, 5))\n",
        "  plt.imshow(attn, aspect=\"auto\", origin=\"upper\")\n",
        "  plt.colorbar()\n",
        "  plt.title(f\"Attention Heatmap: Layer {layer_idx}\")\n",
        "\n",
        "  # For HuggingFace `tokenizers` library: tokenizer.id_to_token(id)\n",
        "  token_strs = [tokenizer.id_to_token(t) for t in seq_tokens_cpu]\n",
        "\n",
        "  T = len(token_strs)\n",
        "  plt.xticks(range(T), token_strs, rotation=90, fontsize=6)\n",
        "  plt.yticks(range(T), token_strs, fontsize=6)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "G6vOTnSyhTvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention(0)"
      ],
      "metadata": {
        "id": "sJ7Bssmc7t6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention(1)"
      ],
      "metadata": {
        "id": "wZO-04d-7z9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Validation Perplexity"
      ],
      "metadata": {
        "id": "BjOEE8pt6rJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(val_ppl_list, label=\"Val perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Validation Perplexity\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ezFxCt5rhdVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Sample generations"
      ],
      "metadata": {
        "id": "VjBj14NQ_MeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0):\n",
        "    model_was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Encode prompt into token IDs\n",
        "    input_ids = tokenizer.encode(prompt).ids\n",
        "\n",
        "    # Keep only the last SEQ_LEN tokens if the prompt is long\n",
        "    input_ids = input_ids[-SEQ_LEN:]\n",
        "\n",
        "    # (1, T)\n",
        "    x = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Maintain the sliding window\n",
        "            if x.size(1) > SEQ_LEN:\n",
        "                x = x[:, -SEQ_LEN:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            # Take logits for the last position\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample next token ID\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append token\n",
        "            x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "    # Convert tensor back to Python list\n",
        "    out_ids = x[0].cpu().tolist()\n",
        "\n",
        "    # Extract only the generated continuation\n",
        "    gen_ids = out_ids[len(input_ids):]\n",
        "\n",
        "    # Decode IDs into text\n",
        "    gen_text = tokenizer.decode(gen_ids)\n",
        "\n",
        "    if model_was_training:\n",
        "        model.train()\n",
        "\n",
        "    return gen_text"
      ],
      "metadata": {
        "id": "fi_cIpzMhe-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ROMEO:\\n\"\n",
        "output = generate_text(model, tokenizer, prompt, max_new_tokens=80, temperature=0.8)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-noCgR1vhir0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"CLAUDIO:\\n\"\n",
        "output = generate_text(model, tokenizer, prompt, max_new_tokens=80, temperature=0.8)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "H5x5Cvnvm0IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Help!:\\n\"\n",
        "output = generate_text(model, tokenizer, prompt, max_new_tokens=80, temperature=0.8)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "slMVDk7pp5MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I love:\\n\"\n",
        "output = generate_text(model, tokenizer, prompt, max_new_tokens=80, temperature=0.8)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-A50EsG40eFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}