{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWY-7_iWEzwl"
      },
      "source": [
        "# Assignment 3: Transformer is All You Need\n",
        "\n",
        "Federico Giorgi (fg2617)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrVJ4E7Ezwn"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DckZ6MKIEzwo"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries\n",
        "import os, math, torch, urllib.request\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Data Preparation"
      ],
      "metadata": {
        "id": "Wvn5fnwPL9ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Load the Tiny Shakespeare text"
      ],
      "metadata": {
        "id": "VMDIvziNMA00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define local path for dataset\n",
        "TINY_PATH = \"tiny_shakespeare.txt\"\n",
        "\n",
        "# Download the Tiny Shakespeare corpus from Karpathyâ€™s repo\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "    TINY_PATH\n",
        ")\n",
        "\n",
        "# Read the corpus into memory as a single string\n",
        "with open(TINY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Print the first 100 characters\n",
        "print(corpus_text[:100])"
      ],
      "metadata": {
        "id": "NOFCpTZyi2-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a16f2e2-11bb-4e9d-9dac-f4eea0d095db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Tokenization"
      ],
      "metadata": {
        "id": "8Gd-yoPcMFxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special tokens for padding, unknown words, and sequence boundaries\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "\n",
        "# Initialize a Byte Pair Encoding (BPE) tokenizer with an unknown token\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Split text initially on whitespace before BPE merges are applied\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the BPE tokenizer on the corpus, limiting vocabulary size to 500 tokens\n",
        "trainer = BpeTrainer(vocab_size=500, min_frequency=2, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator([corpus_text], trainer=trainer)\n",
        "\n",
        "# Retrieve token IDs for the special tokens and vocabulary size\n",
        "pad_id  = tokenizer.token_to_id(\"[PAD]\")\n",
        "bos_id  = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id  = tokenizer.token_to_id(\"[EOS]\")\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# Encode entire corpus to integer IDs\n",
        "ids = tokenizer.encode(corpus_text).ids"
      ],
      "metadata": {
        "id": "DUVdr42OL6tl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Sequence formatting"
      ],
      "metadata": {
        "id": "qPNi6UI_OIGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For next-token prediction: Input = first N tokens, Target = same sequence shifted by 1\n",
        "SEQ_LEN = 50\n",
        "\n",
        "def make_windows(token_ids, seq_len):\n",
        "    # produce (inp, tgt) pairs with stride=1, overlapping\n",
        "    # last complete window ends at len-1 to allow shift\n",
        "    L = len(token_ids)\n",
        "    # Need inp length = seq_len, tgt length = seq_len, so we need i .. i+seq_len for inp and i+1 .. i+seq_len+1 for tgt\n",
        "    # That means i must go until L - (seq_len + 1)\n",
        "    limit = L - (seq_len + 1)\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, max(0, limit + 1)):\n",
        "        seq = token_ids[i : i + seq_len + 1]\n",
        "        inp = seq[:-1]\n",
        "        tgt = seq[1:]\n",
        "        inputs.append(inp)\n",
        "        targets.append(tgt)\n",
        "    return inputs, targets\n",
        "\n",
        "inputs, targets = make_windows(ids, SEQ_LEN)"
      ],
      "metadata": {
        "id": "xFfNL--HPOPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 4 Data split"
      ],
      "metadata": {
        "id": "u4uqaaiYPQLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= 3) 80/20 split (on sequence-pairs) =========\n",
        "dataset_size = len(inputs)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size   = dataset_size - train_size\n",
        "\n",
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = [torch.tensor(x, dtype=torch.long) for x in X]\n",
        "        self.Y = [torch.tensor(y, dtype=torch.long) for y in Y]\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
        "\n",
        "full_ds = NextTokenDataset(inputs, targets)\n",
        "train_ds, val_ds = random_split(full_ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))"
      ],
      "metadata": {
        "id": "ngXfQm_NPYYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Token embedding"
      ],
      "metadata": {
        "id": "WvMtw2tGPUPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "def collate_batch(batch):\n",
        "    # All sequences are already length SEQ_LEN, so simple stack\n",
        "    X = torch.stack([b[0] for b in batch], dim=0)  # (B, T)\n",
        "    Y = torch.stack([b[1] for b in batch], dim=0)  # (B, T)\n",
        "    return X, Y\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=collate_batch)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_batch)\n",
        "\n",
        "# ========= 5) Token embeddings + positional encodings =========\n",
        "# Option A (learned positions): nn.Embedding for both tokens and positions\n",
        "class TokenPosEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = nn.Embedding(max_len, d_model)\n",
        "    def forward(self, x):\n",
        "        # x: (B, T) token IDs\n",
        "        B, T = x.size()\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)  # (B, T)\n",
        "        return self.tok(x) + self.pos(pos)  # (B, T, d_model)\n",
        "\n",
        "# Option B (sinusoidal positions): classic transformer-style fixed encodings\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)  # (max_len, d_model)\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, d_model) token embeddings\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)\n",
        "\n",
        "# Example: build embeddings and run one batch through\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "d_model = 256\n",
        "max_len = SEQ_LEN  # since our sequences are fixed-length windows\n",
        "\n",
        "tokpos = TokenPosEmbedding(vocab_size, d_model, max_len).to(device)\n",
        "sinpos = SinusoidalPositionalEncoding(d_model, max_len).to(device)\n",
        "\n",
        "xb, yb = next(iter(train_loader))  # (B, T), (B, T)\n",
        "xb = xb.to(device)\n",
        "emb_tokpos = tokpos(xb)            # (B, T, d_model) learned positions\n",
        "emb_sin    = sinpos(tokpos.tok(xb))# (B, T, d_model) sinusoidal positions added to token embeddings\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n",
        "print(\"Embedded shapes (learned / sinusoidal):\", emb_tokpos.shape, emb_sin.shape)\n",
        "\n",
        "# Your model can now consume emb_tokpos (or emb_sin). For next-token prediction,\n",
        "# typical loss is cross-entropy over logits shaped (B, T, vocab_size) vs targets (B, T).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fz4gMLAhjxh",
        "outputId": "c1d659e0-737b-476d-b04c-de20c0cee7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 500\n",
            "Train batches: 2800 Val batches: 701\n",
            "Embedded shapes (learned / sinusoidal): torch.Size([128, 50, 256]) torch.Size([128, 50, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Tiny Transformer Implementation"
      ],
      "metadata": {
        "id": "-msA__kNPerv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== RMSNorm =====\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-8):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        # x: (..., d_model)\n",
        "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
        "        x_norm = x / rms\n",
        "        return self.weight * x_norm\n",
        "\n",
        "# ===== Sinusoidal Positional Encoding =====\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe)   # (max_len, d_model)\n",
        "    def forward(self, x):                # x: (B,T,D)\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)\n",
        "\n",
        "# ===== Causal Self-Attention =====\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0.0, resid_dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.attn_drop = nn.Dropout(attn_dropout)\n",
        "        self.resid_drop = nn.Dropout(resid_dropout)\n",
        "        self.register_buffer(\"mask_cache\", None, persistent=False)\n",
        "\n",
        "    def _causal_mask(self, T, device):\n",
        "        # Cache an upper-triangular mask of shape (1,1,T,T) with -inf above diagonal\n",
        "        if self.mask_cache is None or self.mask_cache.size(-1) < T:\n",
        "            size = max(T, 1)\n",
        "            m = torch.full((1, 1, size, size), float(\"-inf\"))\n",
        "            m = torch.triu(m, diagonal=1)\n",
        "            self.mask_cache = m.to(device)\n",
        "        return self.mask_cache[:, :, :T, :T]\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, 3, self.n_heads, self.head_dim).transpose(1, 2)  # (B,3,T,H,Hd) -> (B,3,H,T,Hd)\n",
        "        q, k, v = qkv[:,0], qkv[:,1], qkv[:,2]                                       # each: (B,H,T,Hd)\n",
        "\n",
        "        # scaled dot-product attention with causal mask\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)                   # (B,H,T,T)\n",
        "        att = att + self._causal_mask(T, x.device)                                   # prevent attending to future\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v                                                                  # (B,H,T,Hd)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D)                             # (B,T,D)\n",
        "        y = self.proj(y)\n",
        "        return self.resid_drop(y)\n",
        "\n",
        "# ===== MLP / Feed-Forward =====\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, expansion=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        hidden = expansion * d_model\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "# ===== Transformer Block (Pre-norm + RMSNorm) =====\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0.0, resid_dropout=0.0, ff_dropout=0.0, expansion=4):\n",
        "        super().__init__()\n",
        "        self.norm1 = RMSNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, attn_dropout, resid_dropout)\n",
        "        self.norm2 = RMSNorm(d_model)\n",
        "        self.ffn  = FeedForward(d_model, expansion, ff_dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))   # residual + pre-norm\n",
        "        x = x + self.ffn(self.norm2(x))    # residual + pre-norm\n",
        "        return x\n",
        "\n",
        "# ===== Tiny Transformer Language Model =====\n",
        "class TinyTransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4, max_seq_len=SEQ_LEN, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = SinusoidalPositionalEncoding(d_model, max_len=max_seq_len)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, attn_dropout=dropout, resid_dropout=dropout, ff_dropout=dropout, expansion=4)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.norm_f = RMSNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # tie weights (optional, common trick)\n",
        "        self.lm_head.weight = self.tok.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx: (B,T) token ids\n",
        "        x = self.tok(idx)                      # (B,T,D)\n",
        "        x = self.pos(x)                        # add sinusoidal positions\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm_f(x)\n",
        "        logits = self.lm_head(x)               # (B,T,V)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Cross-entropy over vocabulary at each time step\n",
        "            # Flatten to (B*T,V) vs (B*T,)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "# ===== Instantiate model and train =====\n",
        "torch.manual_seed(0)\n",
        "model = TinyTransformerLM(vocab_size=vocab_size, d_model=256, n_layers=4, n_heads=4, max_seq_len=SEQ_LEN, dropout=0.1).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.01)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss, n_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            _, loss = model(xb, yb)\n",
        "            total = xb.numel()\n",
        "            total_loss += loss.item() * total\n",
        "            n_tokens += total\n",
        "    ppl = math.exp(total_loss / n_tokens)\n",
        "    return total_loss / n_tokens, ppl\n",
        "\n",
        "EPOCHS = 2  # raise for better quality\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    tot, count = 0.0, 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        _, loss = model(xb, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        tot += loss.item()\n",
        "        count += 1\n",
        "    val_loss, val_ppl = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch:02d} | train_loss {tot/count:.4f} | val_loss {val_loss:.4f} | val_ppl {val_ppl:.2f}\")\n",
        "\n",
        "# ===== Quick sampling (greedy) =====\n",
        "@torch.no_grad()\n",
        "def generate(prefix_ids, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    x = torch.tensor(prefix_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1,T)\n",
        "    for _ in range(max_new_tokens):\n",
        "        x_in = x[:, -SEQ_LEN:]                      # crop to block size\n",
        "        logits, _ = model(x_in)\n",
        "        next_id = logits[:, -1, :].argmax(dim=-1)   # greedy\n",
        "        x = torch.cat([x, next_id.unsqueeze(1)], dim=1)\n",
        "    return x.squeeze(0).tolist()\n",
        "\n",
        "# Example: take the first training sequence prefix and generate a few tokens\n",
        "sample_inp, _ = next(iter(train_loader))\n",
        "seed = sample_inp[0].tolist()[:20]\n",
        "gen_ids = generate(seed, max_new_tokens=20)\n",
        "\n",
        "print(\"Seed IDs:    \", seed)\n",
        "print(\"Generated IDs\", gen_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SaL12KfziXcJ",
        "outputId": "31c8368f-e050-4116-c26a-073a7568c40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (50) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2515176584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2515176584.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (B,T,V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2515176584.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpansion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# residual + pre-norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# residual + pre-norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2515176584.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# scaled dot-product attention with causal mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# (B,H,T,T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_causal_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m                                   \u001b[0;31m# prevent attending to future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (50) at non-singleton dimension 3"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}