{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWY-7_iWEzwl"
      },
      "source": [
        "# Assignment 3: Transformer is All You Need\n",
        "\n",
        "Federico Giorgi (fg2617)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrVJ4E7Ezwn"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DckZ6MKIEzwo"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries\n",
        "import os, math, torch\n",
        "from dataclasses import dataclass\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= 0) Load Tiny Shakespeare =========\n",
        "# If you already have it locally, set TINY_PATH to that file.\n",
        "# Otherwise, download once from the classic URL.\n",
        "TINY_PATH = \"tiny_shakespeare.txt\"\n",
        "if not os.path.exists(TINY_PATH):\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "        TINY_PATH\n",
        "    )\n",
        "\n",
        "with open(TINY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# ========= 1) Train a subword tokenizer (BPE) with vocab <= 500 =========\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(vocab_size=500, min_frequency=2, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator([corpus_text], trainer=trainer)\n",
        "\n",
        "pad_id  = tokenizer.token_to_id(\"[PAD]\")\n",
        "bos_id  = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id  = tokenizer.token_to_id(\"[EOS]\")\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# Encode entire corpus to integer IDs\n",
        "ids = tokenizer.encode(corpus_text).ids\n",
        "\n",
        "# ========= 2) Sequence formatting: overlapping fixed-length windows =========\n",
        "# For next-token prediction: Input = first N tokens, Target = same sequence shifted by 1\n",
        "SEQ_LEN = 50  # N in your spec\n",
        "def make_windows(token_ids, seq_len):\n",
        "    # produce (inp, tgt) pairs with stride=1, overlapping\n",
        "    # last complete window ends at len-1 to allow shift\n",
        "    L = len(token_ids)\n",
        "    # Need inp length = seq_len, tgt length = seq_len, so we need i .. i+seq_len for inp and i+1 .. i+seq_len+1 for tgt\n",
        "    # That means i must go until L - (seq_len + 1)\n",
        "    limit = L - (seq_len + 1)\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, max(0, limit + 1)):\n",
        "        seq = token_ids[i : i + seq_len + 1]\n",
        "        inp = seq[:-1]\n",
        "        tgt = seq[1:]\n",
        "        inputs.append(inp)\n",
        "        targets.append(tgt)\n",
        "    return inputs, targets\n",
        "\n",
        "inputs, targets = make_windows(ids, SEQ_LEN)\n",
        "\n",
        "# ========= 3) 80/20 split (on sequence-pairs) =========\n",
        "dataset_size = len(inputs)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size   = dataset_size - train_size\n",
        "\n",
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = [torch.tensor(x, dtype=torch.long) for x in X]\n",
        "        self.Y = [torch.tensor(y, dtype=torch.long) for y in Y]\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
        "\n",
        "full_ds = NextTokenDataset(inputs, targets)\n",
        "train_ds, val_ds = random_split(full_ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# ========= 4) DataLoaders =========\n",
        "BATCH_SIZE = 128\n",
        "def collate_batch(batch):\n",
        "    # All sequences are already length SEQ_LEN, so simple stack\n",
        "    X = torch.stack([b[0] for b in batch], dim=0)  # (B, T)\n",
        "    Y = torch.stack([b[1] for b in batch], dim=0)  # (B, T)\n",
        "    return X, Y\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=collate_batch)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_batch)\n",
        "\n",
        "# ========= 5) Token embeddings + positional encodings =========\n",
        "# Option A (learned positions): nn.Embedding for both tokens and positions\n",
        "class TokenPosEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = nn.Embedding(max_len, d_model)\n",
        "    def forward(self, x):\n",
        "        # x: (B, T) token IDs\n",
        "        B, T = x.size()\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)  # (B, T)\n",
        "        return self.tok(x) + self.pos(pos)  # (B, T, d_model)\n",
        "\n",
        "# Option B (sinusoidal positions): classic transformer-style fixed encodings\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)  # (max_len, d_model)\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, d_model) token embeddings\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)\n",
        "\n",
        "# Example: build embeddings and run one batch through\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "d_model = 256\n",
        "max_len = SEQ_LEN  # since our sequences are fixed-length windows\n",
        "\n",
        "tokpos = TokenPosEmbedding(vocab_size, d_model, max_len).to(device)\n",
        "sinpos = SinusoidalPositionalEncoding(d_model, max_len).to(device)\n",
        "\n",
        "xb, yb = next(iter(train_loader))  # (B, T), (B, T)\n",
        "xb = xb.to(device)\n",
        "emb_tokpos = tokpos(xb)            # (B, T, d_model) learned positions\n",
        "emb_sin    = sinpos(tokpos.tok(xb))# (B, T, d_model) sinusoidal positions added to token embeddings\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n",
        "print(\"Embedded shapes (learned / sinusoidal):\", emb_tokpos.shape, emb_sin.shape)\n",
        "\n",
        "# Your model can now consume emb_tokpos (or emb_sin). For next-token prediction,\n",
        "# typical loss is cross-entropy over logits shaped (B, T, vocab_size) vs targets (B, T).\n"
      ],
      "metadata": {
        "id": "3Fz4gMLAhjxh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}