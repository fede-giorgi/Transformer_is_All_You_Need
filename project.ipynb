{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWY-7_iWEzwl"
      },
      "source": [
        "# Assignment 3: Transformer is All You Need\n",
        "\n",
        "Federico Giorgi (fg2617)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrVJ4E7Ezwn"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "DckZ6MKIEzwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b18df16-6cd7-4d13-96c5-9a7e3d151970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import all the libraries\n",
        "import os, math, torch, urllib.request\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel, Whitespace\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Data Preparation"
      ],
      "metadata": {
        "id": "Wvn5fnwPL9ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Load the Tiny Shakespeare text\n",
        "\n",
        "The Tiny Shakespeare corpus is downloaded directly from Karpathy’s repository and loaded into memory as a single UTF-8 string. Printing the initial characters serves as a basic integrity check to ensure the file was retrieved correctly."
      ],
      "metadata": {
        "id": "VMDIvziNMA00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define local path for dataset\n",
        "TINY_PATH = \"tiny_shakespeare.txt\"\n",
        "\n",
        "# Download the Tiny Shakespeare corpus from Karpathy’s repo\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "    TINY_PATH\n",
        ")\n",
        "\n",
        "# Read the corpus into memory as a single string\n",
        "with open(TINY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Print the first 100 characters\n",
        "print(corpus_text[:100])"
      ],
      "metadata": {
        "id": "NOFCpTZyi2-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc947c13-4694-41fc-d40c-699360371bc1"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Tokenization\n",
        "\n",
        "A Byte Pair Encoding (BPE) tokenizer with a vocabulary capped at 500 tokens is trained on the corpus. Special tokens for padding, unknown symbols, and sequence boundaries are included. After training, the entire corpus is converted into integer token ids, forming the numerical representation used by the model."
      ],
      "metadata": {
        "id": "8Gd-yoPcMFxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special tokens for padding, unknown words, and sequence boundaries\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "\n",
        "# Initialize a Byte Pair Encoding (BPE) tokenizer with an unknown token\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Split text initially on whitespace before BPE merges are applied\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "# Train the BPE tokenizer on the corpus, limiting vocabulary size to 500 tokens\n",
        "trainer = BpeTrainer(vocab_size=500, min_frequency=2, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator([corpus_text], trainer=trainer)\n",
        "\n",
        "# Retrieve token IDs for the special tokens and vocabulary size\n",
        "pad_id  = tokenizer.token_to_id(\"[PAD]\")\n",
        "bos_id  = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id  = tokenizer.token_to_id(\"[EOS]\")\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# Encode entire corpus to integer IDs\n",
        "ids = tokenizer.encode(corpus_text).ids"
      ],
      "metadata": {
        "id": "DUVdr42OL6tl"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sanity check 1: encode/decode consistency\")\n",
        "\n",
        "for start in [0, 1000, 5000]:\n",
        "    snippet = corpus_text[start:start+120]\n",
        "    decoded = tokenizer.decode(tokenizer.encode(snippet).ids)\n",
        "\n",
        "    print(\"\\n--- Original ---\")\n",
        "    print(snippet)\n",
        "\n",
        "    print(\"\\n--- Decoded ---\")\n",
        "    print(decoded)"
      ],
      "metadata": {
        "id": "p6oAfJAEndDm",
        "outputId": "66360645-a340-4687-e058-0fc9f088a2a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check 1: encode/decode consistency\n",
            "\n",
            "--- Original ---\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved ra\n",
            "\n",
            "--- Decoded ---\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved ra\n",
            "\n",
            "--- Original ---\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the comm\n",
            "\n",
            "--- Decoded ---\n",
            " Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the comm\n",
            "\n",
            "--- Original ---\n",
            "or heart, the arm our soldier,\n",
            "Our steed the leg, the tongue our trumpeter.\n",
            "With other muniments and petty helps\n",
            "In this\n",
            "\n",
            "--- Decoded ---\n",
            " or heart, the arm our soldier,\n",
            "Our steed the leg, the tongue our trumpeter.\n",
            "With other muniments and petty helps\n",
            "In this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSanity check 2: ID preservation check\")\n",
        "\n",
        "snippet = corpus_text[2000:2200]\n",
        "\n",
        "ids_1   = tokenizer.encode(snippet).ids\n",
        "decoded = tokenizer.decode(ids_1)\n",
        "ids_2   = tokenizer.encode(decoded).ids\n",
        "\n",
        "print(\"IDs identical? \", ids_1 == ids_2)\n"
      ],
      "metadata": {
        "id": "_GYTzeulpfXA",
        "outputId": "c82b2fda-b146-4a59-f9a2-1819f3916f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sanity check 2: ID preservation check\n",
            "IDs identical?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Sequence formatting\n",
        "\n",
        "For next-token prediction, the tokenized text is split into overlapping fixed-length windows.\n",
        "Each window provides an input sequence of length T and a corresponding target sequence shifted by one position. This produces many short supervised examples capturing local context within the corpus."
      ],
      "metadata": {
        "id": "qPNi6UI_OIGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For next-token prediction: Input = first N tokens, Target = same sequence shifted by 1\n",
        "\n",
        "def make_windows(token_ids, seq_len):\n",
        "    # Total number of tokens in the corpus\n",
        "    L = len(token_ids)\n",
        "\n",
        "    # We need i + seq_len + 1 <= L so that both input and target fit\n",
        "    limit = L - (seq_len + 1)\n",
        "\n",
        "    # Lists to collect all input and target sequences\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Slide a window of size (seq_len + 1) across the corpus with stride 1\n",
        "    for i in range(0, max(0, limit + 1)):\n",
        "        # Take a chunk of length seq_len + 1\n",
        "        seq = token_ids[i : i + seq_len + 1]\n",
        "\n",
        "        # Input: all tokens except the last one\n",
        "        inp = seq[:-1]\n",
        "\n",
        "        # Target: all tokens except the first one (shifted by one position)\n",
        "        tgt = seq[1:]\n",
        "\n",
        "        # Append the pair to the dataset lists\n",
        "        inputs.append(inp)\n",
        "        targets.append(tgt)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "# Generate all (input, target) pairs from the encoded corpus\n",
        "SEQ_LEN = 50\n",
        "inputs, targets = make_windows(ids, SEQ_LEN)"
      ],
      "metadata": {
        "id": "xFfNL--HPOPf"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check for the windowed dataset\n",
        "print(f\"Total number of input-target pairs: {len(inputs)}\")\n",
        "\n",
        "# Print the first pair to verify correct shifting\n",
        "print(\"\\nExample pair:\")\n",
        "print(\"Input IDs :\", inputs[0])\n",
        "print(\"Target IDs:\", targets[0])\n",
        "\n",
        "# Decode the first example back to text (optional, for interpretability)\n",
        "print(\"\\nDecoded example:\")\n",
        "print(\"Input text :\", tokenizer.decode(inputs[0]))\n",
        "print(\"Target text:\", tokenizer.decode(targets[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esNl5BCAQ9-6",
        "outputId": "c63c1f5f-79d3-434d-8887-51d10e8d07aa"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of input-target pairs: 517133\n",
            "\n",
            "Example pair:\n",
            "Input IDs : [68, 484, 233, 87, 49, 66, 92, 12, 67, 16, 45, 361, 144, 397, 121, 128, 216, 65, 84, 174, 60, 148, 8, 487, 130, 429, 10, 67, 67, 15, 86, 12, 67, 33, 56, 393, 8, 429, 10, 67, 67, 484, 233, 87, 49, 66, 92, 12, 67, 378]\n",
            "Target IDs: [484, 233, 87, 49, 66, 92, 12, 67, 16, 45, 361, 144, 397, 121, 128, 216, 65, 84, 174, 60, 148, 8, 487, 130, 429, 10, 67, 67, 15, 86, 12, 67, 33, 56, 393, 8, 429, 10, 67, 67, 484, 233, 87, 49, 66, 92, 12, 67, 378, 231]\n",
            "\n",
            "Decoded example:\n",
            "Input text :  First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "Target text: First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 4 Data split\n",
        "\n",
        "The full set of (input, target) pairs is wrapped in a PyTorch Dataset and deterministically divided into an 80/20 train–validation split. Because all sequences have fixed length, batches are created by stacking tensors directly without padding."
      ],
      "metadata": {
        "id": "u4uqaaiYPQLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of (input, target) pairs produced by windowing\n",
        "dataset_size = len(inputs)\n",
        "\n",
        "# 80/20 split\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size   = dataset_size - train_size"
      ],
      "metadata": {
        "id": "odrB0gquRx98"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NextTokenDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Simple Dataset wrapper for next-token prediction.\n",
        "    Stores precomputed windows as LongTensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        # Cast each window to torch.LongTensor (needed by nn.Embedding and CE loss)\n",
        "        self.X = [torch.tensor(x, dtype=torch.long) for x in X]\n",
        "        self.Y = [torch.tensor(y, dtype=torch.long) for y in Y]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of (input, target) pairs\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Return one pair shaped (T,), (T,) where T=SEQ_LEN\n",
        "        return self.X[i], self.Y[i]\n",
        "\n",
        "# Full dataset (deterministic split via fixed seed)\n",
        "full_ds = NextTokenDataset(inputs, targets)\n",
        "train_ds, val_ds = random_split(\n",
        "    full_ds,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DataLoader.\n",
        "    Sequences are already fixed-length (SEQ_LEN), so we can stack them directly.\n",
        "\n",
        "    Input:\n",
        "        batch: list of N items, each is (x, y) with shape (T,), (T,)\n",
        "    Output:\n",
        "        X: (B, T), Y: (B, T)\n",
        "    \"\"\"\n",
        "    # b[0] is input tensor, b[1] is target tensor\n",
        "    X = torch.stack([b[0] for b in batch], dim=0)  # (B, T)\n",
        "    Y = torch.stack([b[1] for b in batch], dim=0)  # (B, T)\n",
        "    return X, Y\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# DataLoaders for training and validation.\n",
        "# Note: drop_last=True on train for consistent batch size (helps with certain training loops).\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "# For validation, keep all examples (no shuffling, no drop_last)\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "ngXfQm_NPYYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity checks\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(f\"Train batches: {len(train_loader)}  |  Val batches: {len(val_loader)}\")\n",
        "print(f\"Batch shapes  -> X: {xb.shape}, Y: {yb.shape}  (expect: ({BATCH_SIZE}, {SEQ_LEN}))\")\n",
        "assert xb.shape == yb.shape == (BATCH_SIZE, SEQ_LEN)"
      ],
      "metadata": {
        "id": "U_rxsSAbSJSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Token embedding\n",
        "\n",
        "Token ids are mapped to continuous vectors through an nn.Embedding layer, producing representations of shape (B,T,d_model). These embeddings form the model’s initial hidden states."
      ],
      "metadata": {
        "id": "WvMtw2tGPUPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 1.5 Token embedding (sinusoidal positions only)\n",
        "\n",
        "# We map token IDs -> dense vectors with nn.Embedding,\n",
        "# then add fixed (non-trainable) sinusoidal positional encodings à la \"Attention Is All You Need\".\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token embedding table: converts token IDs to d_model-dimensional vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T) of dtype long\n",
        "        return self.embed(x)  # (B, T, d_model)\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Fixed sinusoidal positional encodings (no learned parameters).\n",
        "    Follows Vaswani et al. (2017). Register as a buffer so it moves with .to(device)\n",
        "    but is excluded from optimizer updates.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 10000):\n",
        "        super().__init__()\n",
        "        # Create table of shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)        # (max_len, 1)\n",
        "        # Frequencies for even/odd dimensions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "\n",
        "        # Register as buffer (not a Parameter): no gradients, moves with the module across devices\n",
        "        self.register_buffer(\"pe\", pe)  # shape: (max_len, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add positional encodings for the first T positions.\n",
        "        x is expected to be (B, T, d_model).\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        # Slice the first T rows, then broadcast over batch dimension\n",
        "        return x + self.pe[:T].unsqueeze(0)  # (1, T, D) + (B, T, D) -> (B, T, D)\n",
        "\n",
        "\n",
        "# ----- build embeddings and run a quick test on one batch -----\n",
        "d_model  = 256\n",
        "max_len  = max(SEQ_LEN, 512)  # keep some headroom if you later increase context length\n",
        "\n",
        "tok_emb  = TokenEmbedding(vocab_size, d_model).to(device)\n",
        "pos_enc  = SinusoidalPositionalEncoding(d_model, max_len=max_len).to(device)\n",
        "\n",
        "# Fetch one batch of IDs (B, T)\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb = xb.to(device)  # (B, T)\n",
        "\n",
        "# Token embeddings: (B, T, d_model)\n",
        "h_tok = tok_emb(xb)\n",
        "\n",
        "# Add sinusoidal positions: (B, T, d_model)\n",
        "h = pos_enc(h_tok)"
      ],
      "metadata": {
        "id": "3Fz4gMLAhjxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sanity checks for embeddings + sinusoidal positions:\")\n",
        "print(\" - xb dtype / shape:\", xb.dtype, xb.shape)\n",
        "print(\" - token only     :\", h_tok.shape)\n",
        "print(\" - token+position :\", h.shape)\n",
        "\n",
        "# 1) Shapes must match\n",
        "assert h_tok.shape == (xb.size(0), xb.size(1), d_model)\n",
        "assert h.shape     == (xb.size(0), xb.size(1), d_model)\n",
        "\n",
        "# 2) Positional buffer should not require gradients\n",
        "assert pos_enc.pe.requires_grad is False\n",
        "\n",
        "# 3) Same position across different sequences has identical positional vector\n",
        "#    (i.e., h[b, t, :] - h_tok[b, t, :] equals the same vector for all b, fixed t)\n",
        "with torch.no_grad():\n",
        "    t = 0  # check the first position; you can try other t as well\n",
        "    pos_vecs = h[:, t, :] - h_tok[:, t, :]              # (B, D)\n",
        "    diff = (pos_vecs - pos_vecs[0]).abs().max().item()  # max deviation from the first sample\n",
        "    print(f\" - max deviation of pos[t={t}] across batch: {diff:.3e}\")\n",
        "    assert diff < 1e-6\n",
        "\n",
        "# 4) Different positions within the same sequence must have different positional vectors\n",
        "with torch.no_grad():\n",
        "    if xb.size(1) >= 2:\n",
        "        # Compare pos enc at t=0 vs t=1 for the first sample in batch\n",
        "        v0 = h[0, 0, :] - h_tok[0, 0, :]\n",
        "        v1 = h[0, 1, :] - h_tok[0, 1, :]\n",
        "        cosine = torch.nn.functional.cosine_similarity(v0, v1, dim=0).item()\n",
        "        print(f\" - cosine similarity between pos[0] and pos[1]: {cosine:.4f} (should be < 1.0)\")"
      ],
      "metadata": {
        "id": "TYicIHuucuSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Tiny Transformer Implementation"
      ],
      "metadata": {
        "id": "-msA__kNPerv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Transformer Structure"
      ],
      "metadata": {
        "id": "YvEVjqIfgWLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention with causal masking\"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        scores = Q @ K.transpose(-2, -1) / (d_model ** 0.5)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
        "\n",
        "        output = attn_weights @ V\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Two-layer MLP\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"One transformer block: Attention + FFN + Residuals\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.attention = SelfAttention(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, attn_weights = self.attention(x)\n",
        "        x = x + attn_out  # Residual\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = x + ffn_out  # Residual\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"Complete transformer model\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_layers, d_ff, max_seq_len=128):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, max_seq_len, d_model) * 0.02)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, d_ff) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        x = self.token_embedding(x) + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        all_attn = []\n",
        "        for block in self.blocks:\n",
        "            x, attn = block(x)\n",
        "            all_attn.append(attn)\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "        return logits, all_attn"
      ],
      "metadata": {
        "id": "SaL12KfziXcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Transformer Training"
      ],
      "metadata": {
        "id": "MWu-27bZgaYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 128\n",
        "n_layers = 2\n",
        "d_ff = 512\n",
        "learning_rate = 1e-3\n",
        "n_epochs = 20  # per iniziare, poi aumenti\n",
        "batch_size = BATCH_SIZE  # già definito sopra\n",
        "\n",
        "model = TinyTransformer(vocab_size, d_model, n_layers, d_ff, max_seq_len=SEQ_LEN).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# mixed precision solo se c'è GPU\n",
        "use_amp = device.type == \"cuda\"\n",
        "scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Embedding dim: {d_model}\")\n",
        "print(f\"Layers: {n_layers}\")\n",
        "print(f\"FFN hidden: {d_ff}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Epochs: {n_epochs}\")\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_ppl_list = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)            # (B, T)\n",
        "        yb = yb.to(device)            # (B, T)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(enabled=use_amp):\n",
        "            logits, _ = model(xb)     # (B, T, vocab_size)\n",
        "            # facciamo next-token prediction su T posizioni\n",
        "            loss = criterion(\n",
        "                logits.view(-1, vocab_size),\n",
        "                yb.view(-1)\n",
        "            )\n",
        "\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # valutazione su validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits, _ = model(xb)\n",
        "            loss = criterion(\n",
        "                logits.view(-1, vocab_size),\n",
        "                yb.view(-1)\n",
        "            )\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_ppl = math.exp(avg_val_loss)\n",
        "    print(f\"Epoch {epoch+1:3d}/{n_epochs} | train loss: {avg_train_loss:.4f} | \"\n",
        "        f\"val loss: {avg_val_loss:.4f} | val PPL: {val_ppl:.2f}\")\n",
        "    val_ppl = math.exp(avg_val_loss)\n",
        "    val_ppl_list.append(val_ppl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:3d}/{n_epochs} | train loss: {avg_train_loss:.4f} | val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining finished!\")\n",
        "\n",
        "final_val_loss = val_losses[-1]\n",
        "final_val_ppl = math.exp(final_val_loss)\n",
        "print(f\"\\nFinal validation loss: {final_val_loss:.4f}\")\n",
        "print(f\"Final validation perplexity: {final_val_ppl:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NEXfmXg0avlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Attention heatmap per una sequenza di esempio\n",
        "# ============================================================\n",
        "\n",
        "model.eval()\n",
        "xb, yb = next(iter(val_loader))   # prendo un batch dal validation\n",
        "xb = xb.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits, all_attn = model(xb)\n",
        "\n",
        "# Scegliamo il primo esempio nel batch\n",
        "seq_tokens = xb[0]            # shape: (T,)\n",
        "seq_tokens_cpu = seq_tokens.cpu().tolist()\n",
        "\n",
        "# Scegliamo il layer di cui mostrare l'attenzione (es. il primo)\n",
        "layer_idx = 0\n",
        "attn = all_attn[layer_idx][0]     # shape: (T, T) per il primo esempio\n",
        "attn = attn.detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(attn, aspect=\"auto\", origin=\"lower\")\n",
        "plt.colorbar()\n",
        "plt.title(f\"Attention heatmap - layer {layer_idx}\")\n",
        "\n",
        "# Se hai un tokenizer con id→token, puoi mettere le etichette sugli assi\n",
        "try:\n",
        "    # adattalo in base alla tua API del tokenizer\n",
        "    # per HuggingFace tokenizers:\n",
        "    #   tokenizer.id_to_token(token_id)\n",
        "    token_strs = [tokenizer.id_to_token(t) for t in seq_tokens_cpu]\n",
        "    # Oppure, se hai un metodo diverso:\n",
        "    # token_strs = tokenizer.decode(seq_tokens_cpu).split()\n",
        "\n",
        "    T = len(token_strs)\n",
        "    plt.xticks(range(T), token_strs, rotation=90, fontsize=6)\n",
        "    plt.yticks(range(T), token_strs, fontsize=6)\n",
        "except Exception as e:\n",
        "    print(\"Non sono riuscito a mettere le etichette dei token sull'heatmap:\", e)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G6vOTnSyhTvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Visualization & Interpretation"
      ],
      "metadata": {
        "id": "CTtm-_smaueJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Training and validation loss curves"
      ],
      "metadata": {
        "id": "PgxvBLZpgeX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(train_losses, label=\"Train loss\")\n",
        "plt.plot(val_losses, label=\"Val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w8p0MRFqc8OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(val_ppl_list, label=\"Val perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Validation Perplexity\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ezFxCt5rhdVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ENCODE DEL PROMPT\n",
        "    # Se usi `tokenizers`:\n",
        "    input_ids = tokenizer.encode(prompt).ids        # <--- importante: .ids\n",
        "\n",
        "    # Taglio alla lunghezza massima\n",
        "    input_ids = input_ids[-SEQ_LEN:]\n",
        "    x = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > SEQ_LEN:\n",
        "            x = x[:, -SEQ_LEN:]   # mantieni solo gli ultimi SEQ_LEN token\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(x)\n",
        "            logits = logits[:, -1, :]  # solo l'ultimo passo\n",
        "            if temperature != 1.0:\n",
        "                logits = logits / temperature\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)  # sampling\n",
        "            # oppure greedy:\n",
        "            # next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "    out_ids = x[0].cpu().tolist()\n",
        "\n",
        "    # DECODIFICA:\n",
        "    # versione 1: prompt + generato\n",
        "    full_text = tokenizer.decode(out_ids)\n",
        "\n",
        "    # versione 2: solo la parte generata oltre il prompt\n",
        "    gen_ids = out_ids[len(input_ids):]\n",
        "    gen_text = tokenizer.decode(gen_ids)\n",
        "\n",
        "    return full_text, gen_text\n"
      ],
      "metadata": {
        "id": "fi_cIpzMhe-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ROMEO:\\n\"\n",
        "full_text, gen_text = generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8)\n",
        "\n",
        "print(gen_text)"
      ],
      "metadata": {
        "id": "-noCgR1vhir0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"CLAUDIO:\\n\"\n",
        "full_text, gen_text = generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8)\n",
        "\n",
        "print(gen_text)"
      ],
      "metadata": {
        "id": "H5x5Cvnvm0IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Help!:\\n\"\n",
        "full_text, gen_text = generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8)\n",
        "\n",
        "print(gen_text)"
      ],
      "metadata": {
        "id": "slMVDk7pp5MU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}